{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\n",
    "        \"Ollama not running. Launch ollama before proceeding.\"\n",
    "    )\n",
    "print(\"Ollama running:\", check_if_running(\"ollama\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "file_path = \"instruction-data-with-response.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "     test_data = json.load(file)\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a local Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the query_model function defined earlier, we can evaluate the responses generated by our fine-tuned model that prompts the gemma2:2b model to rate our finetuned modelâ€™s responses on a scale from 0 to 100 based on the given test set\n",
    "response as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def query_model(\n",
    "    prompt, \n",
    "    model=\"gemma2:2b\", \n",
    "    url=\"http://localhost:11434/api/chat\"\n",
    "):\n",
    "    data = { \n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"options\": { \n",
    "        \"seed\": 123,\n",
    "        \"temperature\": 0,\n",
    "        \"num_ctx\": 2048\n",
    "    }\n",
    "    }\n",
    "    payload = json.dumps(data).encode(\"utf-8\") \n",
    "    request = urllib.request.Request( \n",
    "        url, \n",
    "        data=payload, \n",
    "        method=\"POST\" \n",
    "    )\n",
    "    \n",
    "    request.add_header(\"Content-Type\", \"application/json\") \n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response: \n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily eat plants!  Here's a breakdown of their diet:\n",
      "\n",
      "**Main Food Sources:**\n",
      "\n",
      "* **Grasses:** This is the staple food for llamas. They love to munch on various types of grasses like Timothy grass, orchard grass, and bromegrass. \n",
      "* **Hay:** Dried hay provides additional nutrition during colder months or when access to fresh pasture is limited.  \n",
      "* **Forbs:** These are flowering plants that offer a variety of nutrients.\n",
      "\n",
      "**Other Occasional Foods:**\n",
      "\n",
      "* **Leaves:** Llamas will eat leaves from trees and shrubs, but this should be done in moderation as they can be less nutritious than grasses. \n",
      "* **Fruits & Vegetables:** In some cases, llamas may enjoy fruits like apples or carrots, but these are not essential parts of their diet.  \n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "* **Fresh Water:** Llamas need access to fresh water at all times. \n",
      "* **Variety:** Offering a variety of grasses and forbs helps ensure they get the full spectrum of nutrients. \n",
      "* **Quality Hay:** High-quality hay is crucial, as it provides essential energy and protein.  \n",
      "* **Avoid Toxic Plants:** Llamas should not be allowed to eat certain plants that are toxic to them, such as rhododendrons or yew trees.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions about llamas! ðŸ¦™ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = \"gemma2:2b\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> I would give the model's response a score of **75/100**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Strengths:** The simile \"The car is as fast as a cheetah\" effectively compares the speed of the car to that of a cheetah, which is known for its incredible speed. \n",
      "* **Weaknesses:**  While this simile works well, it could be even stronger with some slight adjustments. \n",
      "\n",
      "**Here's how we can improve the response:**\n",
      "\n",
      "The car is as fast as a **bolt of lightning**. This comparison highlights the suddenness and explosive power of the car's speed.\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Specificity:**  Using \"bolt of lightning\" adds a more vivid image to the simile, emphasizing the car's swiftness. \n",
      "* **Impact:** The comparison evokes a sense of awe and wonder at the car's incredible speed.\n",
      "\n",
      "\n",
      "\n",
      "Overall, the model's response is good but could be even better with some minor tweaks. \n",
      "\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus cloudside product.\n",
      "\n",
      "Score:\n",
      ">> The model's response is incorrect and deserves a score of **20/100**. Here's why:\n",
      "\n",
      "* **Incorrect Information:** The statement \"cumulus cloudside product\" is not a valid or accurate description of thunderstorm clouds.  \n",
      "* **Lack of Clarity:** The response lacks clarity and precision in its explanation. \n",
      "\n",
      "\n",
      "**Here's how to improve the model's response:**\n",
      "\n",
      "The correct answer is: **The type of cloud typically associated with thunderstorms is cumulonimbus.** \n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Cumulonimbus clouds** are towering, anvil-shaped clouds that are a hallmark of thunderstorms. They develop vertically and often produce heavy rain, hail, lightning, and strong winds.  \n",
      "* **Other types of clouds:** While other cloud types can be associated with storms (like stratus or nimbostratus), cumulonimbus is the most prominent type directly linked to thunderstorms. \n",
      "\n",
      "\n",
      "Let me know if you'd like more information about thunderstorm clouds! \n",
      "\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is George Orwell.\n",
      "\n",
      "Score:\n",
      ">> The answer is **Jane Austen**.  \n",
      "\n",
      "**Score: 0/100** \n",
      "\n",
      "The provided response is incorrect. The model has failed to correctly identify the author of \"Pride and Prejudice.\" \n",
      "\n",
      "\n",
      "Here's why the model needs improvement:\n",
      "\n",
      "* **Lack of Knowledge:** The model should have access to a vast database of literary works and their authors.\n",
      "* **Accuracy:**  It must be able to verify information against reliable sources. \n",
      "* **Understanding Context:** It needs to understand that \"Pride and Prejudice\" is a classic novel by Jane Austen.\n",
      "\n",
      "\n",
      "**To improve the model's accuracy, you could:**\n",
      "\n",
      "* **Train it on a larger dataset of literary works and their authors.** This would help it learn more about literature.\n",
      "* **Implement fact-checking mechanisms.**  This would ensure that the model only provides accurate information. \n",
      "* **Use external APIs for knowledge retrieval.** These APIs can provide access to vast databases of information, including literary works and their authors. \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore how to improve this model further! \n",
      "\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt, model))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the instruction fine-tuning LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the average score by instructing the model to respond in only integer answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_scores(json_data, json_key, model=\"gemma2:2b\"):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \"\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Respond with the integer number only.\"    \n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "        continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 110/110 [05:34<00:00,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 110 of 110\n",
      "Average score: 56.10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
