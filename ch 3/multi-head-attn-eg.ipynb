{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following explaination is generated using gpt-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through an example with specific dimensions to see how the matrix dimensions change from input to output in the `MultiHeadAttention` class.\n",
    "\n",
    "Assume the following dimensions:\n",
    "- `batch_size (b) = 2`\n",
    "- `num_tokens = 4`\n",
    "- `d_in = 8`\n",
    "- `d_out = 12`\n",
    "- `num_heads = 3`\n",
    "\n",
    "Given these, the `head_dim` would be `d_out // num_heads = 12 // 3 = 4`.\n",
    "\n",
    "### Step-by-Step Dimension Changes\n",
    "\n",
    "1. **Input `x`**:\n",
    "   - Shape: `(batch_size, num_tokens, d_in)`\n",
    "   - Example: `(2, 4, 8)`\n",
    "\n",
    "2. **Linear Transformations (W_query, W_key, W_value)**:\n",
    "   - Each of these layers projects `d_in` to `d_out`.\n",
    "   - Shape after transformation: `(batch_size, num_tokens, d_out)`\n",
    "   - Example: `(2, 4, 12)`\n",
    "\n",
    "3. **Reshape for Multi-Head Attention**:\n",
    "   - Reshape to separate heads: `(batch_size, num_tokens, num_heads, head_dim)`\n",
    "   - Example: `(2, 4, 3, 4)`\n",
    "\n",
    "4. **Transpose for Attention Calculation**:\n",
    "   - Transpose to bring `num_heads` to the second dimension: `(batch_size, num_heads, num_tokens, head_dim)`\n",
    "   - Example: `(2, 3, 4, 4)`\n",
    "\n",
    "5. **Attention Scores Calculation**:\n",
    "   - Compute attention scores: `queries @ keys.transpose(-2, -1)`\n",
    "   - Shape: `(batch_size, num_heads, num_tokens, num_tokens)`\n",
    "   - Example: `(2, 3, 4, 4)`\n",
    "\n",
    "6. **Attention Weights and Context Vector**:\n",
    "   - Apply softmax and dropout to get attention weights.\n",
    "   - Multiply attention weights with values to get context vectors.\n",
    "   - Shape: `(batch_size, num_heads, num_tokens, head_dim)`\n",
    "   - Example: `(2, 3, 4, 4)`\n",
    "\n",
    "7. **Concatenate Heads**:\n",
    "   - Transpose back and concatenate heads: `(batch_size, num_tokens, num_heads * head_dim)`\n",
    "   - Example: `(2, 4, 12)`\n",
    "\n",
    "8. **Final Projection (`out_proj`)**:\n",
    "   - Project concatenated output back to `d_out`.\n",
    "   - Shape: `(batch_size, num_tokens, d_out)`\n",
    "   - Example: `(2, 4, 12)`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Numbers\n",
    "\n",
    "Let's assume a simple input tensor `x` with shape `(2, 4, 8)`: \n",
    "\n",
    "\n",
    "```python\n",
    "   x = torch.randn(2, 4, 8)\n",
    "   ```\n",
    "1. **After Linear Transformations**:\n",
    "   ```python\n",
    "   queries = self.W_query(x)  # Shape: (2, 4, 12)\n",
    "   keys = self.W_key(x)       # Shape: (2, 4, 12)\n",
    "   values = self.W_value(x)   # Shape: (2, 4, 12)\n",
    "   ```\n",
    "\n",
    "2. **Reshape for Multi-Head Attention**:\n",
    "   ```python\n",
    "   queries = queries.view(2, 4, 3, 4)  # Shape: (2, 4, 3, 4)\n",
    "   keys = keys.view(2, 4, 3, 4)        # Shape: (2, 4, 3, 4)\n",
    "   values = values.view(2, 4, 3, 4)    # Shape: (2, 4, 3, 4)\n",
    "   ```\n",
    "\n",
    "3. **Transpose for Attention Calculation**:\n",
    "   ```python\n",
    "   queries = queries.transpose(1, 2)  # Shape: (2, 3, 4, 4)\n",
    "   keys = keys.transpose(1, 2)        # Shape: (2, 3, 4, 4)\n",
    "   values = values.transpose(1, 2)    # Shape: (2, 3, 4, 4)\n",
    "   ```\n",
    "\n",
    "4. **Attention Scores Calculation**:\n",
    "   ```python\n",
    "   attn_scores = queries @ keys.transpose(-2, -1)  # Shape: (2, 3, 4, 4)\n",
    "   ```\n",
    "\n",
    "5. **Attention Weights and Context Vector**:\n",
    "   ```python\n",
    "   attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "   context_vec = attn_weights @ values  # Shape: (2, 3, 4, 4)\n",
    "   ```\n",
    "\n",
    "6. **Concatenate Heads**:\n",
    "   ```python\n",
    "   context_vec = context_vec.transpose(1, 2).contiguous().view(2, 4, 12)  # Shape: (2, 4, 12)\n",
    "   ```\n",
    "\n",
    "7. **Final Projection (`out_proj`)**:\n",
    "   ```python\n",
    "   context_vec = self.out_proj(context_vec)  # Shape: (2, 4, 12)\n",
    "   ```\n",
    "\n",
    "This final output `context_vec` has the same shape as the desired output dimension `(batch_size, num_tokens, d_out)`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
